{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring DLPack APIs and semantics\n",
    "\n",
    "_Note: installing all these libraries in a single environment is a little painful. Here is a way that works (as of now, YMMV): https://gist.github.com/rgommers/347b40695b526ff3993a61d36bdb1c6e_.\n",
    "\n",
    "For the relevant part of the API standard doc, see: https://data-apis.github.io/array-api/latest/design_topics/data_interchange.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python APIs for each library\n",
    "\n",
    "Create a simple int32 array, and roundtrip with `to/from_dlpack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # CuPy\n",
    "import cupy as cp\n",
    "\n",
    "x = cp.arange(3)\n",
    "capsule = x.toDlpack()\n",
    "x2 = cp.fromDlpack(capsule)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 2.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MXNet\n",
    "import mxnet\n",
    "\n",
    "x = mxnet.nd.arange(3)\n",
    "# MXNet also has to_dlpack_for_write(), with identical docs (?)\n",
    "# Looks like the same idea as JAX: keep ownership if _for_read(),\n",
    "#                                  consume if _for_write().\n",
    "capsule = x.to_dlpack_for_read()\n",
    "x2 = mxnet.nd.from_dlpack(capsule)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.utils.dlpack\n",
    "\n",
    "x = torch.arange(3)\n",
    "capsule = torch.utils.dlpack.to_dlpack(x)\n",
    "x2 = torch.utils.dlpack.from_dlpack(capsule)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(3)\n",
    "capsule = tf.experimental.dlpack.to_dlpack(x)\n",
    "x2 = tf.experimental.dlpack.from_dlpack(capsule)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.dlpack\n",
    "\n",
    "x = jax.numpy.arange(3)\n",
    "# Note: take_ownership=False (default) requires jaxlib 0.1.57, released 11 Nov 2020\n",
    "#       this is a mode where the user guarantees not to mutate the buffer\n",
    "#       see https://github.com/google/jax/issues/4636\n",
    "capsule = jax.dlpack.to_dlpack(x, take_ownership=True)\n",
    "x2 = jax.dlpack.from_dlpack(capsule)\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some observations\n",
    "    \n",
    "- All libraries except NumPy and Dask support DLPack, with a similar API\n",
    "- Names are not the same:\n",
    "  - `to_dlpack`, `from_dlpack` *functions* (PyTorch, TensorFlow, JAX)\n",
    "  - `toDlpack`, `fromDlpack` *methods* (CuPy)\n",
    "  - a `take_ownership` keyword (JAX)\n",
    "  - `to_dlpack_for_read`/`to_dlpack_for_write` *method* + `from_dlpack` *function* (MXNet)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interop between libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow - PyTorch interop\n",
    "# ----------------------------\n",
    "import tensorflow as tf\n",
    "import torch.utils.dlpack\n",
    "\n",
    "x = tf.range(3)\n",
    "capsule = tf.experimental.dlpack.to_dlpack(x)\n",
    "x2 = torch.utils.dlpack.from_dlpack(capsule)\n",
    "\n",
    "x2 += 1\n",
    "assert x2[2] == 3  # sanity check we got the data\n",
    "\n",
    "capsule2 = torch.utils.dlpack.to_dlpack(x2)\n",
    "x3 = tf.experimental.dlpack.from_dlpack(capsule2)\n",
    "\n",
    "assert x3[2] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch to CuPy\n",
    "# ```````````````\n",
    "x = cp.arange(3)\n",
    "capsule = x.toDlpack()\n",
    "t2 = torch.utils.dlpack.from_dlpack(capsule)\n",
    "\n",
    "# This will actually share memory:\n",
    "t2[0] = 3\n",
    "assert x[0] == 3\n",
    "\n",
    "# Now see if `x` is still available after `t2` goes out of scope:\n",
    "x = cp.arange(3)\n",
    "\n",
    "def somefunc(x):\n",
    "    capsule = x.toDlpack()\n",
    "    t2 = torch.utils.dlpack.from_dlpack(capsule)\n",
    "    t2[0] = 3\n",
    "    return None\n",
    "\n",
    "somefunc(x)\n",
    "x += 1\n",
    "assert x[0] == 4  # Yep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid argument: Invalid buffer passed to Execute() as argument 0 to replica 0: Invalid argument: Hold requested on deleted or donated buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7d46df7e645a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Now see the import of JAX's immutability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx)\u001b[0m\n\u001b[1;32m   3954\u001b[0m   \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3955\u001b[0m   \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_index_for_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3956\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3958\u001b[0m \u001b[0;31m# TODO(phawkins): re-enable jit after fixing excessive recompilation for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx)\u001b[0m\n\u001b[1;32m   3972\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3973\u001b[0m     y = lax.gather(y, indexer.gather_indices, indexer.dnums,\n\u001b[0;32m-> 3974\u001b[0;31m                    indexer.gather_slice_shape)\n\u001b[0m\u001b[1;32m   3975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3976\u001b[0m   \u001b[0;31m# Reverses axes with negative strides.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(operand, start_indices, dimension_numbers, slice_sizes)\u001b[0m\n\u001b[1;32m    841\u001b[0m   return gather_p.bind(\n\u001b[1;32m    842\u001b[0m       \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension_numbers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       slice_sizes=canonicalize_shape(slice_sizes))\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/many-libs/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled_primitive\u001b[0;34m(prim, compiled, result_handler, *args)\u001b[0m\n\u001b[1;32m    347\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid argument: Invalid buffer passed to Execute() as argument 0 to replica 0: Invalid argument: Hold requested on deleted or donated buffer"
     ]
    }
   ],
   "source": [
    "# JAX to PyTorch\n",
    "# ``````````````\n",
    "j = jax.numpy.arange(3)\n",
    "capsule = jax.dlpack.to_dlpack(j, take_ownership=True)\n",
    "t = torch.utils.dlpack.from_dlpack(capsule)\n",
    "\n",
    "# Now see the impact of JAX's immutability\n",
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "from_dlpack received an invalid capsule. Note that DLTensor capsules can be consumed only once, so you might have already constructed a tensor from it once.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0d6fb99267e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcapsule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dlpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtake_ownership\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dlpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dlpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: from_dlpack received an invalid capsule. Note that DLTensor capsules can be consumed only once, so you might have already constructed a tensor from it once."
     ]
    }
   ],
   "source": [
    "# Consuming a capsule twice is a no-no!\n",
    "j = jax.numpy.arange(3)\n",
    "capsule = jax.dlpack.to_dlpack(j, take_ownership=True)\n",
    "t = torch.utils.dlpack.from_dlpack(capsule)\n",
    "t2 = torch.utils.dlpack.from_dlpack(capsule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does an implementation look like?\n",
    "\n",
    "A CuPy implementation snippet (only some essential parts) from https://github.com/cupy/cupy/blob/master/cupy/core/dlpack.pyx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cdef class DLPackMemory(memory.BaseMemory):\n",
    "    \"\"\"Memory object for a dlpack tensor.\n",
    "    \n",
    "    This does not allocate any memory.\n",
    "    \"\"\"\n",
    "    cdef DLManagedTensor* dlm_tensor\n",
    "    cdef object dltensor\n",
    "\n",
    "    def __init__(self, object dltensor):\n",
    "        self.dltensor = dltensor\n",
    "        self.dlm_tensor = <DLManagedTensor *>cpython.PyCapsule_GetPointer(\n",
    "            dltensor, 'dltensor')\n",
    "        ...\n",
    "        # Make sure this capsule will never be used again.\n",
    "        cpython.PyCapsule_SetName(dltensor, 'used_dltensor')\n",
    "\n",
    "    def __dealloc__(self):\n",
    "        self.dlm_tensor.deleter(self.dlm_tensor)\n",
    "\n",
    "\n",
    "cpdef ndarray fromDlpack(object dltensor):\n",
    "    \"\"\"Zero-copy conversion from a DLPack tensor to a :class:`~cupy.ndarray`.\"\"\"\n",
    "    mem = DLPackMemory(dltensor)\n",
    "    ...\n",
    "    return ndarray(shape_vec, cp_dtype, mem_ptr, strides=strides_vec)\n",
    "\n",
    "\n",
    "cpdef object toDlpack(ndarray array) except +:\n",
    "    cdef DLManagedTensor* dlm_tensor = \\\n",
    "        <DLManagedTensor*>stdlib.malloc(sizeof(DLManagedTensor))\n",
    "\n",
    "    cdef size_t ndim = array._shape.size()\n",
    "    cdef DLTensor* dl_tensor = &dlm_tensor.dl_tensor\n",
    "    dl_tensor.data = array.data.ptr\n",
    "    dl_tensor.ndim = ndim\n",
    "\n",
    "    ...\n",
    "\n",
    "    dlm_tensor.manager_ctx = <void *>array\n",
    "    cpython.Py_INCREF(array)\n",
    "    dlm_tensor.deleter = deleter\n",
    "\n",
    "    return cpython.PyCapsule_New(dlm_tensor, 'dltensor', pycapsule_deleter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what's going on with ownership and deletion?\n",
    "\n",
    "_Key bits of explanation by Tianqi Chen (from https://github.com/data-apis/consortium-feedback/issues/1)_:\n",
    "\n",
    "Clarification wrt \"consume exactly one\": It does not mean that we are moving the memory from numpy to torch. Instead, the convention means that the PyCapsule can only be consumed exactly once. The exporter(that calls `to_dlpack`) still retains the memory.\n",
    "\n",
    "... \n",
    "\n",
    "The memory will be released only after both `x` (exported tensor) and `t2` (imported tensor) go out of scope.\n",
    "\n",
    "...\n",
    "\n",
    "In particular, the `DLManagedTensor` contains a deleter that allows the consumer to signal that the tensor is no longer needed. Because the way the signature is designed, we need to make sure that there is a sole consumer of the `DLManagedTensor` so it is only called once when the consumer no longer needs the memory (otherwise it will cause a double free).\n",
    "\n",
    "Of course, we can also change the signature to include refcounting(e.g. call `IncRef` when there is a copy) in `DLManagedTensor`, however, that means additional requirement that not every exporter might support.\n",
    "\n",
    "...\n",
    "\n",
    "The way things works is that when the consumer choose to de-allocate later, it will call into the deleter in the `DLManagedTensor`. A common implementation of a deleter will then decrease the refcount to the array object.\n",
    "\n",
    "For example, in order to implement `np.to_dlpack`, we will call `PyIncRef` on the numpy object, and put the object pointer into the `manager_ctx` field. Then the deleter will call into `PyDecRef`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_My (Ralf's) thoughts on this ownership/deletion behaviour_:\n",
    "\n",
    "- From what I can tell, the issues I saw on issue trackers and the discussions about being careful about ownership are related to the *capsule* only.\n",
    "- This is zero-copy, shared memory behaviour that works as expected.\n",
    "- If the consuming library creates views, it itself is responsible for not letting the base array go out of scope (which would call the deleter) - but that's normal and won't result in unexpected behaviour.\n",
    "- Producing libraries *may* decide to invalidate the buffer (like JAX does), but they don't *have* to do that. It only matters in case the consumer mutates the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the namespace implementing the array API standard\n",
    "\n",
    "class ndarray():\n",
    "    def __dlpack__(self):\n",
    "        # Export a DLPack capsule\n",
    "        ...\n",
    "        \n",
    "        \n",
    "def from_dlpack(x: array):\n",
    "    # Get capsule\n",
    "    capsule = x.__dlpack__()\n",
    "    \n",
    "    # Construct own array type (here `ndarray) from capsule\n",
    "    # Guarantees that the capsule gets consumed exactly once\n",
    "    ....\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
